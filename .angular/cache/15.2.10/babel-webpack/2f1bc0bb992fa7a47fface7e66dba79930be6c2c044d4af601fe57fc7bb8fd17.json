{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"I:/Angular/angular-blog/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\nconst crypto = require('crypto');\nconst {\n  appendFile,\n  mkdir,\n  readFile,\n  readdir,\n  rm,\n  writeFile\n} = require('fs/promises');\nconst Minipass = require('minipass');\nconst path = require('path');\nconst ssri = require('ssri');\nconst uniqueFilename = require('unique-filename');\nconst contentPath = require('./content/path');\nconst hashToSegments = require('./util/hash-to-segments');\nconst indexV = require('../package.json')['cache-version'].index;\nconst {\n  moveFile\n} = require('@npmcli/fs');\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n};\nmodule.exports.compact = compact;\nfunction compact(_x, _x2, _x3) {\n  return _compact.apply(this, arguments);\n}\nfunction _compact() {\n  _compact = _asyncToGenerator(function* (cache, key, matchFn, opts = {}) {\n    const bucket = bucketPath(cache, key);\n    const entries = yield bucketEntries(bucket);\n    const newEntries = [];\n    // we loop backwards because the bottom-most result is the newest\n    // since we add new entries with appendFile\n    for (let i = entries.length - 1; i >= 0; --i) {\n      const entry = entries[i];\n      // a null integrity could mean either a delete was appended\n      // or the user has simply stored an index that does not map\n      // to any content. we determine if the user wants to keep the\n      // null integrity based on the validateEntry function passed in options.\n      // if the integrity is null and no validateEntry is provided, we break\n      // as we consider the null integrity to be a deletion of everything\n      // that came before it.\n      if (entry.integrity === null && !opts.validateEntry) {\n        break;\n      }\n\n      // if this entry is valid, and it is either the first entry or\n      // the newEntries array doesn't already include an entry that\n      // matches this one based on the provided matchFn, then we add\n      // it to the beginning of our list\n      if ((!opts.validateEntry || opts.validateEntry(entry) === true) && (newEntries.length === 0 || !newEntries.find(oldEntry => matchFn(oldEntry, entry)))) {\n        newEntries.unshift(entry);\n      }\n    }\n    const newIndex = '\\n' + newEntries.map(entry => {\n      const stringified = JSON.stringify(entry);\n      const hash = hashEntry(stringified);\n      return `${hash}\\t${stringified}`;\n    }).join('\\n');\n    const setup = /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix);\n        yield mkdir(path.dirname(target), {\n          recursive: true\n        });\n        return {\n          target,\n          moved: false\n        };\n      });\n      return function setup() {\n        return _ref5.apply(this, arguments);\n      };\n    }();\n    const teardown = /*#__PURE__*/function () {\n      var _ref6 = _asyncToGenerator(function* (tmp) {\n        if (!tmp.moved) {\n          return rm(tmp.target, {\n            recursive: true,\n            force: true\n          });\n        }\n      });\n      return function teardown(_x15) {\n        return _ref6.apply(this, arguments);\n      };\n    }();\n    const write = /*#__PURE__*/function () {\n      var _ref7 = _asyncToGenerator(function* (tmp) {\n        yield writeFile(tmp.target, newIndex, {\n          flag: 'wx'\n        });\n        yield mkdir(path.dirname(bucket), {\n          recursive: true\n        });\n        // we use @npmcli/move-file directly here because we\n        // want to overwrite the existing file\n        yield moveFile(tmp.target, bucket);\n        tmp.moved = true;\n      });\n      return function write(_x16) {\n        return _ref7.apply(this, arguments);\n      };\n    }();\n\n    // write the file atomically\n    const tmp = yield setup();\n    try {\n      yield write(tmp);\n    } finally {\n      yield teardown(tmp);\n    }\n\n    // we reverse the list we generated such that the newest\n    // entries come first in order to make looping through them easier\n    // the true passed to formatEntry tells it to keep null\n    // integrity values, if they made it this far it's because\n    // validateEntry returned true, and as such we should return it\n    return newEntries.reverse().map(entry => formatEntry(cache, entry, true));\n  });\n  return _compact.apply(this, arguments);\n}\nmodule.exports.insert = insert;\nfunction insert(_x4, _x5, _x6) {\n  return _insert.apply(this, arguments);\n}\nfunction _insert() {\n  _insert = _asyncToGenerator(function* (cache, key, integrity, opts = {}) {\n    const {\n      metadata,\n      size\n    } = opts;\n    const bucket = bucketPath(cache, key);\n    const entry = {\n      key,\n      integrity: integrity && ssri.stringify(integrity),\n      time: Date.now(),\n      size,\n      metadata\n    };\n    try {\n      yield mkdir(path.dirname(bucket), {\n        recursive: true\n      });\n      const stringified = JSON.stringify(entry);\n      // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n      yield appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return undefined;\n      }\n      throw err;\n    }\n    return formatEntry(cache, entry);\n  });\n  return _insert.apply(this, arguments);\n}\nmodule.exports.find = find;\nfunction find(_x7, _x8) {\n  return _find.apply(this, arguments);\n}\nfunction _find() {\n  _find = _asyncToGenerator(function* (cache, key) {\n    const bucket = bucketPath(cache, key);\n    try {\n      const entries = yield bucketEntries(bucket);\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key) {\n          return formatEntry(cache, next);\n        } else {\n          return latest;\n        }\n      }, null);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return null;\n      } else {\n        throw err;\n      }\n    }\n  });\n  return _find.apply(this, arguments);\n}\nmodule.exports.delete = del;\nfunction del(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts);\n  }\n  const bucket = bucketPath(cache, key);\n  return rm(bucket, {\n    recursive: true,\n    force: true\n  });\n}\nmodule.exports.lsStream = lsStream;\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = new Minipass({\n    objectMode: true\n  });\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then( /*#__PURE__*/_asyncToGenerator(function* () {\n    const buckets = yield readdirOrEmpty(indexDir);\n    yield Promise.all(buckets.map( /*#__PURE__*/function () {\n      var _ref2 = _asyncToGenerator(function* (bucket) {\n        const bucketPath = path.join(indexDir, bucket);\n        const subbuckets = yield readdirOrEmpty(bucketPath);\n        yield Promise.all(subbuckets.map( /*#__PURE__*/function () {\n          var _ref3 = _asyncToGenerator(function* (subbucket) {\n            const subbucketPath = path.join(bucketPath, subbucket);\n\n            // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n            const subbucketEntries = yield readdirOrEmpty(subbucketPath);\n            yield Promise.all(subbucketEntries.map( /*#__PURE__*/function () {\n              var _ref4 = _asyncToGenerator(function* (entry) {\n                const entryPath = path.join(subbucketPath, entry);\n                try {\n                  const entries = yield bucketEntries(entryPath);\n                  // using a Map here prevents duplicate keys from showing up\n                  // twice, I guess?\n                  const reduced = entries.reduce((acc, entry) => {\n                    acc.set(entry.key, entry);\n                    return acc;\n                  }, new Map());\n                  // reduced is a map of key => entry\n                  for (const entry of reduced.values()) {\n                    const formatted = formatEntry(cache, entry);\n                    if (formatted) {\n                      stream.write(formatted);\n                    }\n                  }\n                } catch (err) {\n                  if (err.code === 'ENOENT') {\n                    return undefined;\n                  }\n                  throw err;\n                }\n              });\n              return function (_x11) {\n                return _ref4.apply(this, arguments);\n              };\n            }()));\n          });\n          return function (_x10) {\n            return _ref3.apply(this, arguments);\n          };\n        }()));\n      });\n      return function (_x9) {\n        return _ref2.apply(this, arguments);\n      };\n    }()));\n    stream.end();\n    return stream;\n  })).catch(err => stream.emit('error', err));\n  return stream;\n}\nmodule.exports.ls = ls;\nfunction ls(_x12) {\n  return _ls.apply(this, arguments);\n}\nfunction _ls() {\n  _ls = _asyncToGenerator(function* (cache) {\n    const entries = yield lsStream(cache).collect();\n    return entries.reduce((acc, xs) => {\n      acc[xs.key] = xs;\n      return acc;\n    }, {});\n  });\n  return _ls.apply(this, arguments);\n}\nmodule.exports.bucketEntries = bucketEntries;\nfunction bucketEntries(_x13, _x14) {\n  return _bucketEntries2.apply(this, arguments);\n}\nfunction _bucketEntries2() {\n  _bucketEntries2 = _asyncToGenerator(function* (bucket, filter) {\n    const data = yield readFile(bucket, 'utf8');\n    return _bucketEntries(data, filter);\n  });\n  return _bucketEntries2.apply(this, arguments);\n}\nfunction _bucketEntries(data, filter) {\n  const entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n    const pieces = entry.split('\\t');\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n    let obj;\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (_) {\n      // eslint-ignore-next-line no-empty-block\n    }\n    // coverage disabled here, no need to test with an entry that parses to something falsey\n    // istanbul ignore else\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\nmodule.exports.bucketDir = bucketDir;\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\nmodule.exports.bucketPath = bucketPath;\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\nmodule.exports.hashKey = hashKey;\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\nmodule.exports.hashEntry = hashEntry;\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\nfunction formatEntry(cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null;\n  }\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\nfunction readdirOrEmpty(dir) {\n  return readdir(dir).catch(err => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return [];\n    }\n    throw err;\n  });\n}","map":{"version":3,"names":["crypto","require","appendFile","mkdir","readFile","readdir","rm","writeFile","Minipass","path","ssri","uniqueFilename","contentPath","hashToSegments","indexV","index","moveFile","module","exports","NotFoundError","Error","constructor","cache","key","code","compact","matchFn","opts","bucket","bucketPath","entries","bucketEntries","newEntries","i","length","entry","integrity","validateEntry","find","oldEntry","unshift","newIndex","map","stringified","JSON","stringify","hash","hashEntry","join","setup","target","tmpPrefix","dirname","recursive","moved","teardown","tmp","force","write","flag","reverse","formatEntry","insert","metadata","size","time","Date","now","err","undefined","reduce","latest","next","delete","del","removeFully","lsStream","indexDir","bucketDir","stream","objectMode","Promise","resolve","then","buckets","readdirOrEmpty","all","subbuckets","subbucket","subbucketPath","subbucketEntries","entryPath","reduced","acc","set","Map","values","formatted","end","catch","emit","ls","collect","xs","filter","data","_bucketEntries","split","forEach","pieces","obj","parse","_","push","hashed","hashKey","apply","concat","str","digest","createHash","update","keepAll","dir"],"sources":["I:/Angular/angular-blog/node_modules/cacache/lib/entry-index.js"],"sourcesContent":["'use strict'\n\nconst crypto = require('crypto')\nconst {\n  appendFile,\n  mkdir,\n  readFile,\n  readdir,\n  rm,\n  writeFile,\n} = require('fs/promises')\nconst Minipass = require('minipass')\nconst path = require('path')\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\n\nconst contentPath = require('./content/path')\nconst hashToSegments = require('./util/hash-to-segments')\nconst indexV = require('../package.json')['cache-version'].index\nconst { moveFile } = require('@npmcli/fs')\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nmodule.exports.compact = compact\n\nasync function compact (cache, key, matchFn, opts = {}) {\n  const bucket = bucketPath(cache, key)\n  const entries = await bucketEntries(bucket)\n  const newEntries = []\n  // we loop backwards because the bottom-most result is the newest\n  // since we add new entries with appendFile\n  for (let i = entries.length - 1; i >= 0; --i) {\n    const entry = entries[i]\n    // a null integrity could mean either a delete was appended\n    // or the user has simply stored an index that does not map\n    // to any content. we determine if the user wants to keep the\n    // null integrity based on the validateEntry function passed in options.\n    // if the integrity is null and no validateEntry is provided, we break\n    // as we consider the null integrity to be a deletion of everything\n    // that came before it.\n    if (entry.integrity === null && !opts.validateEntry) {\n      break\n    }\n\n    // if this entry is valid, and it is either the first entry or\n    // the newEntries array doesn't already include an entry that\n    // matches this one based on the provided matchFn, then we add\n    // it to the beginning of our list\n    if ((!opts.validateEntry || opts.validateEntry(entry) === true) &&\n      (newEntries.length === 0 ||\n        !newEntries.find((oldEntry) => matchFn(oldEntry, entry)))) {\n      newEntries.unshift(entry)\n    }\n  }\n\n  const newIndex = '\\n' + newEntries.map((entry) => {\n    const stringified = JSON.stringify(entry)\n    const hash = hashEntry(stringified)\n    return `${hash}\\t${stringified}`\n  }).join('\\n')\n\n  const setup = async () => {\n    const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n    await mkdir(path.dirname(target), { recursive: true })\n    return {\n      target,\n      moved: false,\n    }\n  }\n\n  const teardown = async (tmp) => {\n    if (!tmp.moved) {\n      return rm(tmp.target, { recursive: true, force: true })\n    }\n  }\n\n  const write = async (tmp) => {\n    await writeFile(tmp.target, newIndex, { flag: 'wx' })\n    await mkdir(path.dirname(bucket), { recursive: true })\n    // we use @npmcli/move-file directly here because we\n    // want to overwrite the existing file\n    await moveFile(tmp.target, bucket)\n    tmp.moved = true\n  }\n\n  // write the file atomically\n  const tmp = await setup()\n  try {\n    await write(tmp)\n  } finally {\n    await teardown(tmp)\n  }\n\n  // we reverse the list we generated such that the newest\n  // entries come first in order to make looping through them easier\n  // the true passed to formatEntry tells it to keep null\n  // integrity values, if they made it this far it's because\n  // validateEntry returned true, and as such we should return it\n  return newEntries.reverse().map((entry) => formatEntry(cache, entry, true))\n}\n\nmodule.exports.insert = insert\n\nasync function insert (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  try {\n    await mkdir(path.dirname(bucket), { recursive: true })\n    const stringified = JSON.stringify(entry)\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with\n    // this.\n    await appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return undefined\n    }\n\n    throw err\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\n\nasync function find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    const entries = await bucketEntries(bucket)\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.delete = del\n\nfunction del (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rm(bucket, { recursive: true, force: true })\n}\n\nmodule.exports.lsStream = lsStream\n\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = new Minipass({ objectMode: true })\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then(async () => {\n    const buckets = await readdirOrEmpty(indexDir)\n    await Promise.all(buckets.map(async (bucket) => {\n      const bucketPath = path.join(indexDir, bucket)\n      const subbuckets = await readdirOrEmpty(bucketPath)\n      await Promise.all(subbuckets.map(async (subbucket) => {\n        const subbucketPath = path.join(bucketPath, subbucket)\n\n        // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n        const subbucketEntries = await readdirOrEmpty(subbucketPath)\n        await Promise.all(subbucketEntries.map(async (entry) => {\n          const entryPath = path.join(subbucketPath, entry)\n          try {\n            const entries = await bucketEntries(entryPath)\n            // using a Map here prevents duplicate keys from showing up\n            // twice, I guess?\n            const reduced = entries.reduce((acc, entry) => {\n              acc.set(entry.key, entry)\n              return acc\n            }, new Map())\n            // reduced is a map of key => entry\n            for (const entry of reduced.values()) {\n              const formatted = formatEntry(cache, entry)\n              if (formatted) {\n                stream.write(formatted)\n              }\n            }\n          } catch (err) {\n            if (err.code === 'ENOENT') {\n              return undefined\n            }\n            throw err\n          }\n        }))\n      }))\n    }))\n    stream.end()\n    return stream\n  }).catch(err => stream.emit('error', err))\n\n  return stream\n}\n\nmodule.exports.ls = ls\n\nasync function ls (cache) {\n  const entries = await lsStream(cache).collect()\n  return entries.reduce((acc, xs) => {\n    acc[xs.key] = xs\n    return acc\n  }, {})\n}\n\nmodule.exports.bucketEntries = bucketEntries\n\nasync function bucketEntries (bucket, filter) {\n  const data = await readFile(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  const entries = []\n  data.split('\\n').forEach((entry) => {\n    if (!entry) {\n      return\n    }\n\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (_) {\n      // eslint-ignore-next-line no-empty-block\n    }\n    // coverage disabled here, no need to test with an entry that parses to something falsey\n    // istanbul ignore else\n    if (obj) {\n      entries.push(obj)\n    }\n  })\n  return entries\n}\n\nmodule.exports.bucketDir = bucketDir\n\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports.bucketPath = bucketPath\n\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(\n    path,\n    [bucketDir(cache)].concat(hashToSegments(hashed))\n  )\n}\n\nmodule.exports.hashKey = hashKey\n\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports.hashEntry = hashEntry\n\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata,\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return readdir(dir).catch((err) => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return []\n    }\n\n    throw err\n  })\n}\n"],"mappings":"AAAA,YAAY;;AAAA;AAEZ,MAAMA,MAAM,GAAGC,OAAO,CAAC,QAAQ,CAAC;AAChC,MAAM;EACJC,UAAU;EACVC,KAAK;EACLC,QAAQ;EACRC,OAAO;EACPC,EAAE;EACFC;AACF,CAAC,GAAGN,OAAO,CAAC,aAAa,CAAC;AAC1B,MAAMO,QAAQ,GAAGP,OAAO,CAAC,UAAU,CAAC;AACpC,MAAMQ,IAAI,GAAGR,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMS,IAAI,GAAGT,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMU,cAAc,GAAGV,OAAO,CAAC,iBAAiB,CAAC;AAEjD,MAAMW,WAAW,GAAGX,OAAO,CAAC,gBAAgB,CAAC;AAC7C,MAAMY,cAAc,GAAGZ,OAAO,CAAC,yBAAyB,CAAC;AACzD,MAAMa,MAAM,GAAGb,OAAO,CAAC,iBAAiB,CAAC,CAAC,eAAe,CAAC,CAACc,KAAK;AAChE,MAAM;EAAEC;AAAS,CAAC,GAAGf,OAAO,CAAC,YAAY,CAAC;AAE1CgB,MAAM,CAACC,OAAO,CAACC,aAAa,GAAG,MAAMA,aAAa,SAASC,KAAK,CAAC;EAC/DC,WAAW,CAAEC,KAAK,EAAEC,GAAG,EAAE;IACvB,KAAK,CAAE,sBAAqBA,GAAI,aAAYD,KAAM,EAAC,CAAC;IACpD,IAAI,CAACE,IAAI,GAAG,QAAQ;IACpB,IAAI,CAACF,KAAK,GAAGA,KAAK;IAClB,IAAI,CAACC,GAAG,GAAGA,GAAG;EAChB;AACF,CAAC;AAEDN,MAAM,CAACC,OAAO,CAACO,OAAO,GAAGA,OAAO;AAAA,SAEjBA,OAAO;EAAA;AAAA;AAAA;EAAA,6BAAtB,WAAwBH,KAAK,EAAEC,GAAG,EAAEG,OAAO,EAAEC,IAAI,GAAG,CAAC,CAAC,EAAE;IACtD,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;IAC3C,MAAMI,UAAU,GAAG,EAAE;IACrB;IACA;IACA,KAAK,IAAIC,CAAC,GAAGH,OAAO,CAACI,MAAM,GAAG,CAAC,EAAED,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;MAC5C,MAAME,KAAK,GAAGL,OAAO,CAACG,CAAC,CAAC;MACxB;MACA;MACA;MACA;MACA;MACA;MACA;MACA,IAAIE,KAAK,CAACC,SAAS,KAAK,IAAI,IAAI,CAACT,IAAI,CAACU,aAAa,EAAE;QACnD;MACF;;MAEA;MACA;MACA;MACA;MACA,IAAI,CAAC,CAACV,IAAI,CAACU,aAAa,IAAIV,IAAI,CAACU,aAAa,CAACF,KAAK,CAAC,KAAK,IAAI,MAC3DH,UAAU,CAACE,MAAM,KAAK,CAAC,IACtB,CAACF,UAAU,CAACM,IAAI,CAAEC,QAAQ,IAAKb,OAAO,CAACa,QAAQ,EAAEJ,KAAK,CAAC,CAAC,CAAC,EAAE;QAC7DH,UAAU,CAACQ,OAAO,CAACL,KAAK,CAAC;MAC3B;IACF;IAEA,MAAMM,QAAQ,GAAG,IAAI,GAAGT,UAAU,CAACU,GAAG,CAAEP,KAAK,IAAK;MAChD,MAAMQ,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC,MAAMW,IAAI,GAAGC,SAAS,CAACJ,WAAW,CAAC;MACnC,OAAQ,GAAEG,IAAK,KAAIH,WAAY,EAAC;IAClC,CAAC,CAAC,CAACK,IAAI,CAAC,IAAI,CAAC;IAEb,MAAMC,KAAK;MAAA,8BAAG,aAAY;QACxB,MAAMC,MAAM,GAAGvC,cAAc,CAACF,IAAI,CAACuC,IAAI,CAAC1B,KAAK,EAAE,KAAK,CAAC,EAAEK,IAAI,CAACwB,SAAS,CAAC;QACtE,MAAMhD,KAAK,CAACM,IAAI,CAAC2C,OAAO,CAACF,MAAM,CAAC,EAAE;UAAEG,SAAS,EAAE;QAAK,CAAC,CAAC;QACtD,OAAO;UACLH,MAAM;UACNI,KAAK,EAAE;QACT,CAAC;MACH,CAAC;MAAA,gBAPKL,KAAK;QAAA;MAAA;IAAA,GAOV;IAED,MAAMM,QAAQ;MAAA,8BAAG,WAAOC,GAAG,EAAK;QAC9B,IAAI,CAACA,GAAG,CAACF,KAAK,EAAE;UACd,OAAOhD,EAAE,CAACkD,GAAG,CAACN,MAAM,EAAE;YAAEG,SAAS,EAAE,IAAI;YAAEI,KAAK,EAAE;UAAK,CAAC,CAAC;QACzD;MACF,CAAC;MAAA,gBAJKF,QAAQ;QAAA;MAAA;IAAA,GAIb;IAED,MAAMG,KAAK;MAAA,8BAAG,WAAOF,GAAG,EAAK;QAC3B,MAAMjD,SAAS,CAACiD,GAAG,CAACN,MAAM,EAAET,QAAQ,EAAE;UAAEkB,IAAI,EAAE;QAAK,CAAC,CAAC;QACrD,MAAMxD,KAAK,CAACM,IAAI,CAAC2C,OAAO,CAACxB,MAAM,CAAC,EAAE;UAAEyB,SAAS,EAAE;QAAK,CAAC,CAAC;QACtD;QACA;QACA,MAAMrC,QAAQ,CAACwC,GAAG,CAACN,MAAM,EAAEtB,MAAM,CAAC;QAClC4B,GAAG,CAACF,KAAK,GAAG,IAAI;MAClB,CAAC;MAAA,gBAPKI,KAAK;QAAA;MAAA;IAAA,GAOV;;IAED;IACA,MAAMF,GAAG,SAASP,KAAK,EAAE;IACzB,IAAI;MACF,MAAMS,KAAK,CAACF,GAAG,CAAC;IAClB,CAAC,SAAS;MACR,MAAMD,QAAQ,CAACC,GAAG,CAAC;IACrB;;IAEA;IACA;IACA;IACA;IACA;IACA,OAAOxB,UAAU,CAAC4B,OAAO,EAAE,CAAClB,GAAG,CAAEP,KAAK,IAAK0B,WAAW,CAACvC,KAAK,EAAEa,KAAK,EAAE,IAAI,CAAC,CAAC;EAC7E,CAAC;EAAA;AAAA;AAEDlB,MAAM,CAACC,OAAO,CAAC4C,MAAM,GAAGA,MAAM;AAAA,SAEfA,MAAM;EAAA;AAAA;AAAA;EAAA,4BAArB,WAAuBxC,KAAK,EAAEC,GAAG,EAAEa,SAAS,EAAET,IAAI,GAAG,CAAC,CAAC,EAAE;IACvD,MAAM;MAAEoC,QAAQ;MAAEC;IAAK,CAAC,GAAGrC,IAAI;IAC/B,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMY,KAAK,GAAG;MACZZ,GAAG;MACHa,SAAS,EAAEA,SAAS,IAAI1B,IAAI,CAACmC,SAAS,CAACT,SAAS,CAAC;MACjD6B,IAAI,EAAEC,IAAI,CAACC,GAAG,EAAE;MAChBH,IAAI;MACJD;IACF,CAAC;IACD,IAAI;MACF,MAAM5D,KAAK,CAACM,IAAI,CAAC2C,OAAO,CAACxB,MAAM,CAAC,EAAE;QAAEyB,SAAS,EAAE;MAAK,CAAC,CAAC;MACtD,MAAMV,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA,MAAMjC,UAAU,CAAC0B,MAAM,EAAG,KAAImB,SAAS,CAACJ,WAAW,CAAE,KAAIA,WAAY,EAAC,CAAC;IACzE,CAAC,CAAC,OAAOyB,GAAG,EAAE;MACZ,IAAIA,GAAG,CAAC5C,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO6C,SAAS;MAClB;MAEA,MAAMD,GAAG;IACX;IACA,OAAOP,WAAW,CAACvC,KAAK,EAAEa,KAAK,CAAC;EAClC,CAAC;EAAA;AAAA;AAEDlB,MAAM,CAACC,OAAO,CAACoB,IAAI,GAAGA,IAAI;AAAA,SAEXA,IAAI;EAAA;AAAA;AAAA;EAAA,0BAAnB,WAAqBhB,KAAK,EAAEC,GAAG,EAAE;IAC/B,MAAMK,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,IAAI;MACF,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;MAC3C,OAAOE,OAAO,CAACwC,MAAM,CAAC,CAACC,MAAM,EAAEC,IAAI,KAAK;QACtC,IAAIA,IAAI,IAAIA,IAAI,CAACjD,GAAG,KAAKA,GAAG,EAAE;UAC5B,OAAOsC,WAAW,CAACvC,KAAK,EAAEkD,IAAI,CAAC;QACjC,CAAC,MAAM;UACL,OAAOD,MAAM;QACf;MACF,CAAC,EAAE,IAAI,CAAC;IACV,CAAC,CAAC,OAAOH,GAAG,EAAE;MACZ,IAAIA,GAAG,CAAC5C,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO,IAAI;MACb,CAAC,MAAM;QACL,MAAM4C,GAAG;MACX;IACF;EACF,CAAC;EAAA;AAAA;AAEDnD,MAAM,CAACC,OAAO,CAACuD,MAAM,GAAGC,GAAG;AAE3B,SAASA,GAAG,CAAEpD,KAAK,EAAEC,GAAG,EAAEI,IAAI,GAAG,CAAC,CAAC,EAAE;EACnC,IAAI,CAACA,IAAI,CAACgD,WAAW,EAAE;IACrB,OAAOb,MAAM,CAACxC,KAAK,EAAEC,GAAG,EAAE,IAAI,EAAEI,IAAI,CAAC;EACvC;EAEA,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;EACrC,OAAOjB,EAAE,CAACsB,MAAM,EAAE;IAAEyB,SAAS,EAAE,IAAI;IAAEI,KAAK,EAAE;EAAK,CAAC,CAAC;AACrD;AAEAxC,MAAM,CAACC,OAAO,CAAC0D,QAAQ,GAAGA,QAAQ;AAElC,SAASA,QAAQ,CAAEtD,KAAK,EAAE;EACxB,MAAMuD,QAAQ,GAAGC,SAAS,CAACxD,KAAK,CAAC;EACjC,MAAMyD,MAAM,GAAG,IAAIvE,QAAQ,CAAC;IAAEwE,UAAU,EAAE;EAAK,CAAC,CAAC;;EAEjD;EACAC,OAAO,CAACC,OAAO,EAAE,CAACC,IAAI,iCAAC,aAAY;IACjC,MAAMC,OAAO,SAASC,cAAc,CAACR,QAAQ,CAAC;IAC9C,MAAMI,OAAO,CAACK,GAAG,CAACF,OAAO,CAAC1C,GAAG;MAAA,8BAAC,WAAOd,MAAM,EAAK;QAC9C,MAAMC,UAAU,GAAGpB,IAAI,CAACuC,IAAI,CAAC6B,QAAQ,EAAEjD,MAAM,CAAC;QAC9C,MAAM2D,UAAU,SAASF,cAAc,CAACxD,UAAU,CAAC;QACnD,MAAMoD,OAAO,CAACK,GAAG,CAACC,UAAU,CAAC7C,GAAG;UAAA,8BAAC,WAAO8C,SAAS,EAAK;YACpD,MAAMC,aAAa,GAAGhF,IAAI,CAACuC,IAAI,CAACnB,UAAU,EAAE2D,SAAS,CAAC;;YAEtD;YACA,MAAME,gBAAgB,SAASL,cAAc,CAACI,aAAa,CAAC;YAC5D,MAAMR,OAAO,CAACK,GAAG,CAACI,gBAAgB,CAAChD,GAAG;cAAA,8BAAC,WAAOP,KAAK,EAAK;gBACtD,MAAMwD,SAAS,GAAGlF,IAAI,CAACuC,IAAI,CAACyC,aAAa,EAAEtD,KAAK,CAAC;gBACjD,IAAI;kBACF,MAAML,OAAO,SAASC,aAAa,CAAC4D,SAAS,CAAC;kBAC9C;kBACA;kBACA,MAAMC,OAAO,GAAG9D,OAAO,CAACwC,MAAM,CAAC,CAACuB,GAAG,EAAE1D,KAAK,KAAK;oBAC7C0D,GAAG,CAACC,GAAG,CAAC3D,KAAK,CAACZ,GAAG,EAAEY,KAAK,CAAC;oBACzB,OAAO0D,GAAG;kBACZ,CAAC,EAAE,IAAIE,GAAG,EAAE,CAAC;kBACb;kBACA,KAAK,MAAM5D,KAAK,IAAIyD,OAAO,CAACI,MAAM,EAAE,EAAE;oBACpC,MAAMC,SAAS,GAAGpC,WAAW,CAACvC,KAAK,EAAEa,KAAK,CAAC;oBAC3C,IAAI8D,SAAS,EAAE;sBACblB,MAAM,CAACrB,KAAK,CAACuC,SAAS,CAAC;oBACzB;kBACF;gBACF,CAAC,CAAC,OAAO7B,GAAG,EAAE;kBACZ,IAAIA,GAAG,CAAC5C,IAAI,KAAK,QAAQ,EAAE;oBACzB,OAAO6C,SAAS;kBAClB;kBACA,MAAMD,GAAG;gBACX;cACF,CAAC;cAAA;gBAAA;cAAA;YAAA,IAAC,CAAC;UACL,CAAC;UAAA;YAAA;UAAA;QAAA,IAAC,CAAC;MACL,CAAC;MAAA;QAAA;MAAA;IAAA,IAAC,CAAC;IACHW,MAAM,CAACmB,GAAG,EAAE;IACZ,OAAOnB,MAAM;EACf,CAAC,EAAC,CAACoB,KAAK,CAAC/B,GAAG,IAAIW,MAAM,CAACqB,IAAI,CAAC,OAAO,EAAEhC,GAAG,CAAC,CAAC;EAE1C,OAAOW,MAAM;AACf;AAEA9D,MAAM,CAACC,OAAO,CAACmF,EAAE,GAAGA,EAAE;AAAA,SAEPA,EAAE;EAAA;AAAA;AAAA;EAAA,wBAAjB,WAAmB/E,KAAK,EAAE;IACxB,MAAMQ,OAAO,SAAS8C,QAAQ,CAACtD,KAAK,CAAC,CAACgF,OAAO,EAAE;IAC/C,OAAOxE,OAAO,CAACwC,MAAM,CAAC,CAACuB,GAAG,EAAEU,EAAE,KAAK;MACjCV,GAAG,CAACU,EAAE,CAAChF,GAAG,CAAC,GAAGgF,EAAE;MAChB,OAAOV,GAAG;IACZ,CAAC,EAAE,CAAC,CAAC,CAAC;EACR,CAAC;EAAA;AAAA;AAED5E,MAAM,CAACC,OAAO,CAACa,aAAa,GAAGA,aAAa;AAAA,SAE7BA,aAAa;EAAA;AAAA;AAAA;EAAA,oCAA5B,WAA8BH,MAAM,EAAE4E,MAAM,EAAE;IAC5C,MAAMC,IAAI,SAASrG,QAAQ,CAACwB,MAAM,EAAE,MAAM,CAAC;IAC3C,OAAO8E,cAAc,CAACD,IAAI,EAAED,MAAM,CAAC;EACrC,CAAC;EAAA;AAAA;AAED,SAASE,cAAc,CAAED,IAAI,EAAED,MAAM,EAAE;EACrC,MAAM1E,OAAO,GAAG,EAAE;EAClB2E,IAAI,CAACE,KAAK,CAAC,IAAI,CAAC,CAACC,OAAO,CAAEzE,KAAK,IAAK;IAClC,IAAI,CAACA,KAAK,EAAE;MACV;IACF;IAEA,MAAM0E,MAAM,GAAG1E,KAAK,CAACwE,KAAK,CAAC,IAAI,CAAC;IAChC,IAAI,CAACE,MAAM,CAAC,CAAC,CAAC,IAAI9D,SAAS,CAAC8D,MAAM,CAAC,CAAC,CAAC,CAAC,KAAKA,MAAM,CAAC,CAAC,CAAC,EAAE;MACpD;MACA;MACA;IACF;IACA,IAAIC,GAAG;IACP,IAAI;MACFA,GAAG,GAAGlE,IAAI,CAACmE,KAAK,CAACF,MAAM,CAAC,CAAC,CAAC,CAAC;IAC7B,CAAC,CAAC,OAAOG,CAAC,EAAE;MACV;IAAA;IAEF;IACA;IACA,IAAIF,GAAG,EAAE;MACPhF,OAAO,CAACmF,IAAI,CAACH,GAAG,CAAC;IACnB;EACF,CAAC,CAAC;EACF,OAAOhF,OAAO;AAChB;AAEAb,MAAM,CAACC,OAAO,CAAC4D,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAAS,CAAExD,KAAK,EAAE;EACzB,OAAOb,IAAI,CAACuC,IAAI,CAAC1B,KAAK,EAAG,UAASR,MAAO,EAAC,CAAC;AAC7C;AAEAG,MAAM,CAACC,OAAO,CAACW,UAAU,GAAGA,UAAU;AAEtC,SAASA,UAAU,CAAEP,KAAK,EAAEC,GAAG,EAAE;EAC/B,MAAM2F,MAAM,GAAGC,OAAO,CAAC5F,GAAG,CAAC;EAC3B,OAAOd,IAAI,CAACuC,IAAI,CAACoE,KAAK,CACpB3G,IAAI,EACJ,CAACqE,SAAS,CAACxD,KAAK,CAAC,CAAC,CAAC+F,MAAM,CAACxG,cAAc,CAACqG,MAAM,CAAC,CAAC,CAClD;AACH;AAEAjG,MAAM,CAACC,OAAO,CAACiG,OAAO,GAAGA,OAAO;AAEhC,SAASA,OAAO,CAAE5F,GAAG,EAAE;EACrB,OAAOuB,IAAI,CAACvB,GAAG,EAAE,QAAQ,CAAC;AAC5B;AAEAN,MAAM,CAACC,OAAO,CAAC6B,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAAS,CAAEuE,GAAG,EAAE;EACvB,OAAOxE,IAAI,CAACwE,GAAG,EAAE,MAAM,CAAC;AAC1B;AAEA,SAASxE,IAAI,CAAEwE,GAAG,EAAEC,MAAM,EAAE;EAC1B,OAAOvH,MAAM,CACVwH,UAAU,CAACD,MAAM,CAAC,CAClBE,MAAM,CAACH,GAAG,CAAC,CACXC,MAAM,CAAC,KAAK,CAAC;AAClB;AAEA,SAAS1D,WAAW,CAAEvC,KAAK,EAAEa,KAAK,EAAEuF,OAAO,EAAE;EAC3C;EACA,IAAI,CAACvF,KAAK,CAACC,SAAS,IAAI,CAACsF,OAAO,EAAE;IAChC,OAAO,IAAI;EACb;EAEA,OAAO;IACLnG,GAAG,EAAEY,KAAK,CAACZ,GAAG;IACda,SAAS,EAAED,KAAK,CAACC,SAAS;IAC1B3B,IAAI,EAAE0B,KAAK,CAACC,SAAS,GAAGxB,WAAW,CAACU,KAAK,EAAEa,KAAK,CAACC,SAAS,CAAC,GAAGiC,SAAS;IACvEL,IAAI,EAAE7B,KAAK,CAAC6B,IAAI;IAChBC,IAAI,EAAE9B,KAAK,CAAC8B,IAAI;IAChBF,QAAQ,EAAE5B,KAAK,CAAC4B;EAClB,CAAC;AACH;AAEA,SAASsB,cAAc,CAAEsC,GAAG,EAAE;EAC5B,OAAOtH,OAAO,CAACsH,GAAG,CAAC,CAACxB,KAAK,CAAE/B,GAAG,IAAK;IACjC,IAAIA,GAAG,CAAC5C,IAAI,KAAK,QAAQ,IAAI4C,GAAG,CAAC5C,IAAI,KAAK,SAAS,EAAE;MACnD,OAAO,EAAE;IACX;IAEA,MAAM4C,GAAG;EACX,CAAC,CAAC;AACJ"},"metadata":{},"sourceType":"script","externalDependencies":[]}