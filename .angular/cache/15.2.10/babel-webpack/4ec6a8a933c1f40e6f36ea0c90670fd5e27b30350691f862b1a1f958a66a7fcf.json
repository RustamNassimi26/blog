{"ast":null,"code":"'use strict';\n\nvar _asyncToGenerator = require(\"I:/Angular/angular-blog/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\nconst util = require('util');\nconst crypto = require('crypto');\nconst fs = require('@npmcli/fs');\nconst Minipass = require('minipass');\nconst path = require('path');\nconst ssri = require('ssri');\nconst uniqueFilename = require('unique-filename');\nconst contentPath = require('./content/path');\nconst fixOwner = require('./util/fix-owner');\nconst hashToSegments = require('./util/hash-to-segments');\nconst indexV = require('../package.json')['cache-version'].index;\nconst moveFile = require('@npmcli/move-file');\nconst _rimraf = require('rimraf');\nconst rimraf = util.promisify(_rimraf);\nrimraf.sync = _rimraf.sync;\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`);\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n};\nmodule.exports.compact = compact;\nfunction compact(_x, _x2, _x3) {\n  return _compact.apply(this, arguments);\n}\nfunction _compact() {\n  _compact = _asyncToGenerator(function* (cache, key, matchFn, opts = {}) {\n    const bucket = bucketPath(cache, key);\n    const entries = yield bucketEntries(bucket);\n    const newEntries = [];\n    // we loop backwards because the bottom-most result is the newest\n    // since we add new entries with appendFile\n    for (let i = entries.length - 1; i >= 0; --i) {\n      const entry = entries[i];\n      // a null integrity could mean either a delete was appended\n      // or the user has simply stored an index that does not map\n      // to any content. we determine if the user wants to keep the\n      // null integrity based on the validateEntry function passed in options.\n      // if the integrity is null and no validateEntry is provided, we break\n      // as we consider the null integrity to be a deletion of everything\n      // that came before it.\n      if (entry.integrity === null && !opts.validateEntry) {\n        break;\n      }\n\n      // if this entry is valid, and it is either the first entry or\n      // the newEntries array doesn't already include an entry that\n      // matches this one based on the provided matchFn, then we add\n      // it to the beginning of our list\n      if ((!opts.validateEntry || opts.validateEntry(entry) === true) && (newEntries.length === 0 || !newEntries.find(oldEntry => matchFn(oldEntry, entry)))) {\n        newEntries.unshift(entry);\n      }\n    }\n    const newIndex = '\\n' + newEntries.map(entry => {\n      const stringified = JSON.stringify(entry);\n      const hash = hashEntry(stringified);\n      return `${hash}\\t${stringified}`;\n    }).join('\\n');\n    const setup = /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* () {\n        const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix);\n        yield fixOwner.mkdirfix(cache, path.dirname(target));\n        return {\n          target,\n          moved: false\n        };\n      });\n      return function setup() {\n        return _ref5.apply(this, arguments);\n      };\n    }();\n    const teardown = /*#__PURE__*/function () {\n      var _ref6 = _asyncToGenerator(function* (tmp) {\n        if (!tmp.moved) {\n          return rimraf(tmp.target);\n        }\n      });\n      return function teardown(_x15) {\n        return _ref6.apply(this, arguments);\n      };\n    }();\n    const write = /*#__PURE__*/function () {\n      var _ref7 = _asyncToGenerator(function* (tmp) {\n        yield fs.writeFile(tmp.target, newIndex, {\n          flag: 'wx'\n        });\n        yield fixOwner.mkdirfix(cache, path.dirname(bucket));\n        // we use @npmcli/move-file directly here because we\n        // want to overwrite the existing file\n        yield moveFile(tmp.target, bucket);\n        tmp.moved = true;\n        try {\n          yield fixOwner.chownr(cache, bucket);\n        } catch (err) {\n          if (err.code !== 'ENOENT') {\n            throw err;\n          }\n        }\n      });\n      return function write(_x16) {\n        return _ref7.apply(this, arguments);\n      };\n    }();\n\n    // write the file atomically\n    const tmp = yield setup();\n    try {\n      yield write(tmp);\n    } finally {\n      yield teardown(tmp);\n    }\n\n    // we reverse the list we generated such that the newest\n    // entries come first in order to make looping through them easier\n    // the true passed to formatEntry tells it to keep null\n    // integrity values, if they made it this far it's because\n    // validateEntry returned true, and as such we should return it\n    return newEntries.reverse().map(entry => formatEntry(cache, entry, true));\n  });\n  return _compact.apply(this, arguments);\n}\nmodule.exports.insert = insert;\nfunction insert(_x4, _x5, _x6) {\n  return _insert.apply(this, arguments);\n}\nfunction _insert() {\n  _insert = _asyncToGenerator(function* (cache, key, integrity, opts = {}) {\n    const {\n      metadata,\n      size\n    } = opts;\n    const bucket = bucketPath(cache, key);\n    const entry = {\n      key,\n      integrity: integrity && ssri.stringify(integrity),\n      time: Date.now(),\n      size,\n      metadata\n    };\n    try {\n      yield fixOwner.mkdirfix(cache, path.dirname(bucket));\n      const stringified = JSON.stringify(entry);\n      // NOTE - Cleverness ahoy!\n      //\n      // This works because it's tremendously unlikely for an entry to corrupt\n      // another while still preserving the string length of the JSON in\n      // question. So, we just slap the length in there and verify it on read.\n      //\n      // Thanks to @isaacs for the whiteboarding session that ended up with\n      // this.\n      yield fs.appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n      yield fixOwner.chownr(cache, bucket);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return undefined;\n      }\n      throw err;\n      // There's a class of race conditions that happen when things get deleted\n      // during fixOwner, or between the two mkdirfix/chownr calls.\n      //\n      // It's perfectly fine to just not bother in those cases and lie\n      // that the index entry was written. Because it's a cache.\n    }\n\n    return formatEntry(cache, entry);\n  });\n  return _insert.apply(this, arguments);\n}\nmodule.exports.insert.sync = insertSync;\nfunction insertSync(cache, key, integrity, opts = {}) {\n  const {\n    metadata,\n    size\n  } = opts;\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata\n  };\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket));\n  const stringified = JSON.stringify(entry);\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`);\n  try {\n    fixOwner.chownr.sync(cache, bucket);\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err;\n    }\n  }\n  return formatEntry(cache, entry);\n}\nmodule.exports.find = find;\nfunction find(_x7, _x8) {\n  return _find.apply(this, arguments);\n}\nfunction _find() {\n  _find = _asyncToGenerator(function* (cache, key) {\n    const bucket = bucketPath(cache, key);\n    try {\n      const entries = yield bucketEntries(bucket);\n      return entries.reduce((latest, next) => {\n        if (next && next.key === key) {\n          return formatEntry(cache, next);\n        } else {\n          return latest;\n        }\n      }, null);\n    } catch (err) {\n      if (err.code === 'ENOENT') {\n        return null;\n      } else {\n        throw err;\n      }\n    }\n  });\n  return _find.apply(this, arguments);\n}\nmodule.exports.find.sync = findSync;\nfunction findSync(cache, key) {\n  const bucket = bucketPath(cache, key);\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  }\n}\nmodule.exports.delete = del;\nfunction del(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts);\n  }\n  const bucket = bucketPath(cache, key);\n  return rimraf(bucket);\n}\nmodule.exports.delete.sync = delSync;\nfunction delSync(cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insertSync(cache, key, null, opts);\n  }\n  const bucket = bucketPath(cache, key);\n  return rimraf.sync(bucket);\n}\nmodule.exports.lsStream = lsStream;\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = new Minipass({\n    objectMode: true\n  });\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then( /*#__PURE__*/_asyncToGenerator(function* () {\n    const buckets = yield readdirOrEmpty(indexDir);\n    yield Promise.all(buckets.map( /*#__PURE__*/function () {\n      var _ref2 = _asyncToGenerator(function* (bucket) {\n        const bucketPath = path.join(indexDir, bucket);\n        const subbuckets = yield readdirOrEmpty(bucketPath);\n        yield Promise.all(subbuckets.map( /*#__PURE__*/function () {\n          var _ref3 = _asyncToGenerator(function* (subbucket) {\n            const subbucketPath = path.join(bucketPath, subbucket);\n\n            // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n            const subbucketEntries = yield readdirOrEmpty(subbucketPath);\n            yield Promise.all(subbucketEntries.map( /*#__PURE__*/function () {\n              var _ref4 = _asyncToGenerator(function* (entry) {\n                const entryPath = path.join(subbucketPath, entry);\n                try {\n                  const entries = yield bucketEntries(entryPath);\n                  // using a Map here prevents duplicate keys from showing up\n                  // twice, I guess?\n                  const reduced = entries.reduce((acc, entry) => {\n                    acc.set(entry.key, entry);\n                    return acc;\n                  }, new Map());\n                  // reduced is a map of key => entry\n                  for (const entry of reduced.values()) {\n                    const formatted = formatEntry(cache, entry);\n                    if (formatted) {\n                      stream.write(formatted);\n                    }\n                  }\n                } catch (err) {\n                  if (err.code === 'ENOENT') {\n                    return undefined;\n                  }\n                  throw err;\n                }\n              });\n              return function (_x11) {\n                return _ref4.apply(this, arguments);\n              };\n            }()));\n          });\n          return function (_x10) {\n            return _ref3.apply(this, arguments);\n          };\n        }()));\n      });\n      return function (_x9) {\n        return _ref2.apply(this, arguments);\n      };\n    }()));\n    stream.end();\n    return stream;\n  })).catch(err => stream.emit('error', err));\n  return stream;\n}\nmodule.exports.ls = ls;\nfunction ls(_x12) {\n  return _ls.apply(this, arguments);\n}\nfunction _ls() {\n  _ls = _asyncToGenerator(function* (cache) {\n    const entries = yield lsStream(cache).collect();\n    return entries.reduce((acc, xs) => {\n      acc[xs.key] = xs;\n      return acc;\n    }, {});\n  });\n  return _ls.apply(this, arguments);\n}\nmodule.exports.bucketEntries = bucketEntries;\nfunction bucketEntries(_x13, _x14) {\n  return _bucketEntries2.apply(this, arguments);\n}\nfunction _bucketEntries2() {\n  _bucketEntries2 = _asyncToGenerator(function* (bucket, filter) {\n    const data = yield fs.readFile(bucket, 'utf8');\n    return _bucketEntries(data, filter);\n  });\n  return _bucketEntries2.apply(this, arguments);\n}\nmodule.exports.bucketEntries.sync = bucketEntriesSync;\nfunction bucketEntriesSync(bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8');\n  return _bucketEntries(data, filter);\n}\nfunction _bucketEntries(data, filter) {\n  const entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n    const pieces = entry.split('\\t');\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n    let obj;\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (e) {\n      // Entry is corrupted!\n      return;\n    }\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\nmodule.exports.bucketDir = bucketDir;\nfunction bucketDir(cache) {\n  return path.join(cache, `index-v${indexV}`);\n}\nmodule.exports.bucketPath = bucketPath;\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\nmodule.exports.hashKey = hashKey;\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\nmodule.exports.hashEntry = hashEntry;\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\nfunction formatEntry(cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null;\n  }\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\nfunction readdirOrEmpty(dir) {\n  return fs.readdir(dir).catch(err => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return [];\n    }\n    throw err;\n  });\n}","map":{"version":3,"names":["util","require","crypto","fs","Minipass","path","ssri","uniqueFilename","contentPath","fixOwner","hashToSegments","indexV","index","moveFile","_rimraf","rimraf","promisify","sync","module","exports","NotFoundError","Error","constructor","cache","key","code","compact","matchFn","opts","bucket","bucketPath","entries","bucketEntries","newEntries","i","length","entry","integrity","validateEntry","find","oldEntry","unshift","newIndex","map","stringified","JSON","stringify","hash","hashEntry","join","setup","target","tmpPrefix","mkdirfix","dirname","moved","teardown","tmp","write","writeFile","flag","chownr","err","reverse","formatEntry","insert","metadata","size","time","Date","now","appendFile","undefined","insertSync","appendFileSync","reduce","latest","next","findSync","bucketEntriesSync","delete","del","removeFully","delSync","lsStream","indexDir","bucketDir","stream","objectMode","Promise","resolve","then","buckets","readdirOrEmpty","all","subbuckets","subbucket","subbucketPath","subbucketEntries","entryPath","reduced","acc","set","Map","values","formatted","end","catch","emit","ls","collect","xs","filter","data","readFile","_bucketEntries","readFileSync","split","forEach","pieces","obj","parse","e","push","hashed","hashKey","apply","concat","str","digest","createHash","update","keepAll","dir","readdir"],"sources":["I:/Angular/angular-blog/node_modules/make-fetch-happen/node_modules/cacache/lib/entry-index.js"],"sourcesContent":["'use strict'\n\nconst util = require('util')\nconst crypto = require('crypto')\nconst fs = require('@npmcli/fs')\nconst Minipass = require('minipass')\nconst path = require('path')\nconst ssri = require('ssri')\nconst uniqueFilename = require('unique-filename')\n\nconst contentPath = require('./content/path')\nconst fixOwner = require('./util/fix-owner')\nconst hashToSegments = require('./util/hash-to-segments')\nconst indexV = require('../package.json')['cache-version'].index\nconst moveFile = require('@npmcli/move-file')\nconst _rimraf = require('rimraf')\nconst rimraf = util.promisify(_rimraf)\nrimraf.sync = _rimraf.sync\n\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor (cache, key) {\n    super(`No cache entry for ${key} found in ${cache}`)\n    this.code = 'ENOENT'\n    this.cache = cache\n    this.key = key\n  }\n}\n\nmodule.exports.compact = compact\n\nasync function compact (cache, key, matchFn, opts = {}) {\n  const bucket = bucketPath(cache, key)\n  const entries = await bucketEntries(bucket)\n  const newEntries = []\n  // we loop backwards because the bottom-most result is the newest\n  // since we add new entries with appendFile\n  for (let i = entries.length - 1; i >= 0; --i) {\n    const entry = entries[i]\n    // a null integrity could mean either a delete was appended\n    // or the user has simply stored an index that does not map\n    // to any content. we determine if the user wants to keep the\n    // null integrity based on the validateEntry function passed in options.\n    // if the integrity is null and no validateEntry is provided, we break\n    // as we consider the null integrity to be a deletion of everything\n    // that came before it.\n    if (entry.integrity === null && !opts.validateEntry) {\n      break\n    }\n\n    // if this entry is valid, and it is either the first entry or\n    // the newEntries array doesn't already include an entry that\n    // matches this one based on the provided matchFn, then we add\n    // it to the beginning of our list\n    if ((!opts.validateEntry || opts.validateEntry(entry) === true) &&\n      (newEntries.length === 0 ||\n        !newEntries.find((oldEntry) => matchFn(oldEntry, entry)))) {\n      newEntries.unshift(entry)\n    }\n  }\n\n  const newIndex = '\\n' + newEntries.map((entry) => {\n    const stringified = JSON.stringify(entry)\n    const hash = hashEntry(stringified)\n    return `${hash}\\t${stringified}`\n  }).join('\\n')\n\n  const setup = async () => {\n    const target = uniqueFilename(path.join(cache, 'tmp'), opts.tmpPrefix)\n    await fixOwner.mkdirfix(cache, path.dirname(target))\n    return {\n      target,\n      moved: false,\n    }\n  }\n\n  const teardown = async (tmp) => {\n    if (!tmp.moved) {\n      return rimraf(tmp.target)\n    }\n  }\n\n  const write = async (tmp) => {\n    await fs.writeFile(tmp.target, newIndex, { flag: 'wx' })\n    await fixOwner.mkdirfix(cache, path.dirname(bucket))\n    // we use @npmcli/move-file directly here because we\n    // want to overwrite the existing file\n    await moveFile(tmp.target, bucket)\n    tmp.moved = true\n    try {\n      await fixOwner.chownr(cache, bucket)\n    } catch (err) {\n      if (err.code !== 'ENOENT') {\n        throw err\n      }\n    }\n  }\n\n  // write the file atomically\n  const tmp = await setup()\n  try {\n    await write(tmp)\n  } finally {\n    await teardown(tmp)\n  }\n\n  // we reverse the list we generated such that the newest\n  // entries come first in order to make looping through them easier\n  // the true passed to formatEntry tells it to keep null\n  // integrity values, if they made it this far it's because\n  // validateEntry returned true, and as such we should return it\n  return newEntries.reverse().map((entry) => formatEntry(cache, entry, true))\n}\n\nmodule.exports.insert = insert\n\nasync function insert (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  try {\n    await fixOwner.mkdirfix(cache, path.dirname(bucket))\n    const stringified = JSON.stringify(entry)\n    // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with\n    // this.\n    await fs.appendFile(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n    await fixOwner.chownr(cache, bucket)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return undefined\n    }\n\n    throw err\n    // There's a class of race conditions that happen when things get deleted\n    // during fixOwner, or between the two mkdirfix/chownr calls.\n    //\n    // It's perfectly fine to just not bother in those cases and lie\n    // that the index entry was written. Because it's a cache.\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.insert.sync = insertSync\n\nfunction insertSync (cache, key, integrity, opts = {}) {\n  const { metadata, size } = opts\n  const bucket = bucketPath(cache, key)\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size,\n    metadata,\n  }\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket))\n  const stringified = JSON.stringify(entry)\n  fs.appendFileSync(bucket, `\\n${hashEntry(stringified)}\\t${stringified}`)\n  try {\n    fixOwner.chownr.sync(cache, bucket)\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err\n    }\n  }\n  return formatEntry(cache, entry)\n}\n\nmodule.exports.find = find\n\nasync function find (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    const entries = await bucketEntries(bucket)\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.find.sync = findSync\n\nfunction findSync (cache, key) {\n  const bucket = bucketPath(cache, key)\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next)\n      } else {\n        return latest\n      }\n    }, null)\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null\n    } else {\n      throw err\n    }\n  }\n}\n\nmodule.exports.delete = del\n\nfunction del (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insert(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rimraf(bucket)\n}\n\nmodule.exports.delete.sync = delSync\n\nfunction delSync (cache, key, opts = {}) {\n  if (!opts.removeFully) {\n    return insertSync(cache, key, null, opts)\n  }\n\n  const bucket = bucketPath(cache, key)\n  return rimraf.sync(bucket)\n}\n\nmodule.exports.lsStream = lsStream\n\nfunction lsStream (cache) {\n  const indexDir = bucketDir(cache)\n  const stream = new Minipass({ objectMode: true })\n\n  // Set all this up to run on the stream and then just return the stream\n  Promise.resolve().then(async () => {\n    const buckets = await readdirOrEmpty(indexDir)\n    await Promise.all(buckets.map(async (bucket) => {\n      const bucketPath = path.join(indexDir, bucket)\n      const subbuckets = await readdirOrEmpty(bucketPath)\n      await Promise.all(subbuckets.map(async (subbucket) => {\n        const subbucketPath = path.join(bucketPath, subbucket)\n\n        // \"/cachename/<bucket 0xFF>/<bucket 0xFF>./*\"\n        const subbucketEntries = await readdirOrEmpty(subbucketPath)\n        await Promise.all(subbucketEntries.map(async (entry) => {\n          const entryPath = path.join(subbucketPath, entry)\n          try {\n            const entries = await bucketEntries(entryPath)\n            // using a Map here prevents duplicate keys from showing up\n            // twice, I guess?\n            const reduced = entries.reduce((acc, entry) => {\n              acc.set(entry.key, entry)\n              return acc\n            }, new Map())\n            // reduced is a map of key => entry\n            for (const entry of reduced.values()) {\n              const formatted = formatEntry(cache, entry)\n              if (formatted) {\n                stream.write(formatted)\n              }\n            }\n          } catch (err) {\n            if (err.code === 'ENOENT') {\n              return undefined\n            }\n            throw err\n          }\n        }))\n      }))\n    }))\n    stream.end()\n    return stream\n  }).catch(err => stream.emit('error', err))\n\n  return stream\n}\n\nmodule.exports.ls = ls\n\nasync function ls (cache) {\n  const entries = await lsStream(cache).collect()\n  return entries.reduce((acc, xs) => {\n    acc[xs.key] = xs\n    return acc\n  }, {})\n}\n\nmodule.exports.bucketEntries = bucketEntries\n\nasync function bucketEntries (bucket, filter) {\n  const data = await fs.readFile(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nmodule.exports.bucketEntries.sync = bucketEntriesSync\n\nfunction bucketEntriesSync (bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8')\n  return _bucketEntries(data, filter)\n}\n\nfunction _bucketEntries (data, filter) {\n  const entries = []\n  data.split('\\n').forEach((entry) => {\n    if (!entry) {\n      return\n    }\n\n    const pieces = entry.split('\\t')\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return\n    }\n    let obj\n    try {\n      obj = JSON.parse(pieces[1])\n    } catch (e) {\n      // Entry is corrupted!\n      return\n    }\n    if (obj) {\n      entries.push(obj)\n    }\n  })\n  return entries\n}\n\nmodule.exports.bucketDir = bucketDir\n\nfunction bucketDir (cache) {\n  return path.join(cache, `index-v${indexV}`)\n}\n\nmodule.exports.bucketPath = bucketPath\n\nfunction bucketPath (cache, key) {\n  const hashed = hashKey(key)\n  return path.join.apply(\n    path,\n    [bucketDir(cache)].concat(hashToSegments(hashed))\n  )\n}\n\nmodule.exports.hashKey = hashKey\n\nfunction hashKey (key) {\n  return hash(key, 'sha256')\n}\n\nmodule.exports.hashEntry = hashEntry\n\nfunction hashEntry (str) {\n  return hash(str, 'sha1')\n}\n\nfunction hash (str, digest) {\n  return crypto\n    .createHash(digest)\n    .update(str)\n    .digest('hex')\n}\n\nfunction formatEntry (cache, entry, keepAll) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity && !keepAll) {\n    return null\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: entry.integrity ? contentPath(cache, entry.integrity) : undefined,\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata,\n  }\n}\n\nfunction readdirOrEmpty (dir) {\n  return fs.readdir(dir).catch((err) => {\n    if (err.code === 'ENOENT' || err.code === 'ENOTDIR') {\n      return []\n    }\n\n    throw err\n  })\n}\n"],"mappings":"AAAA,YAAY;;AAAA;AAEZ,MAAMA,IAAI,GAAGC,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMC,MAAM,GAAGD,OAAO,CAAC,QAAQ,CAAC;AAChC,MAAME,EAAE,GAAGF,OAAO,CAAC,YAAY,CAAC;AAChC,MAAMG,QAAQ,GAAGH,OAAO,CAAC,UAAU,CAAC;AACpC,MAAMI,IAAI,GAAGJ,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMK,IAAI,GAAGL,OAAO,CAAC,MAAM,CAAC;AAC5B,MAAMM,cAAc,GAAGN,OAAO,CAAC,iBAAiB,CAAC;AAEjD,MAAMO,WAAW,GAAGP,OAAO,CAAC,gBAAgB,CAAC;AAC7C,MAAMQ,QAAQ,GAAGR,OAAO,CAAC,kBAAkB,CAAC;AAC5C,MAAMS,cAAc,GAAGT,OAAO,CAAC,yBAAyB,CAAC;AACzD,MAAMU,MAAM,GAAGV,OAAO,CAAC,iBAAiB,CAAC,CAAC,eAAe,CAAC,CAACW,KAAK;AAChE,MAAMC,QAAQ,GAAGZ,OAAO,CAAC,mBAAmB,CAAC;AAC7C,MAAMa,OAAO,GAAGb,OAAO,CAAC,QAAQ,CAAC;AACjC,MAAMc,MAAM,GAAGf,IAAI,CAACgB,SAAS,CAACF,OAAO,CAAC;AACtCC,MAAM,CAACE,IAAI,GAAGH,OAAO,CAACG,IAAI;AAE1BC,MAAM,CAACC,OAAO,CAACC,aAAa,GAAG,MAAMA,aAAa,SAASC,KAAK,CAAC;EAC/DC,WAAW,CAAEC,KAAK,EAAEC,GAAG,EAAE;IACvB,KAAK,CAAE,sBAAqBA,GAAI,aAAYD,KAAM,EAAC,CAAC;IACpD,IAAI,CAACE,IAAI,GAAG,QAAQ;IACpB,IAAI,CAACF,KAAK,GAAGA,KAAK;IAClB,IAAI,CAACC,GAAG,GAAGA,GAAG;EAChB;AACF,CAAC;AAEDN,MAAM,CAACC,OAAO,CAACO,OAAO,GAAGA,OAAO;AAAA,SAEjBA,OAAO;EAAA;AAAA;AAAA;EAAA,6BAAtB,WAAwBH,KAAK,EAAEC,GAAG,EAAEG,OAAO,EAAEC,IAAI,GAAG,CAAC,CAAC,EAAE;IACtD,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;IAC3C,MAAMI,UAAU,GAAG,EAAE;IACrB;IACA;IACA,KAAK,IAAIC,CAAC,GAAGH,OAAO,CAACI,MAAM,GAAG,CAAC,EAAED,CAAC,IAAI,CAAC,EAAE,EAAEA,CAAC,EAAE;MAC5C,MAAME,KAAK,GAAGL,OAAO,CAACG,CAAC,CAAC;MACxB;MACA;MACA;MACA;MACA;MACA;MACA;MACA,IAAIE,KAAK,CAACC,SAAS,KAAK,IAAI,IAAI,CAACT,IAAI,CAACU,aAAa,EAAE;QACnD;MACF;;MAEA;MACA;MACA;MACA;MACA,IAAI,CAAC,CAACV,IAAI,CAACU,aAAa,IAAIV,IAAI,CAACU,aAAa,CAACF,KAAK,CAAC,KAAK,IAAI,MAC3DH,UAAU,CAACE,MAAM,KAAK,CAAC,IACtB,CAACF,UAAU,CAACM,IAAI,CAAEC,QAAQ,IAAKb,OAAO,CAACa,QAAQ,EAAEJ,KAAK,CAAC,CAAC,CAAC,EAAE;QAC7DH,UAAU,CAACQ,OAAO,CAACL,KAAK,CAAC;MAC3B;IACF;IAEA,MAAMM,QAAQ,GAAG,IAAI,GAAGT,UAAU,CAACU,GAAG,CAAEP,KAAK,IAAK;MAChD,MAAMQ,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC,MAAMW,IAAI,GAAGC,SAAS,CAACJ,WAAW,CAAC;MACnC,OAAQ,GAAEG,IAAK,KAAIH,WAAY,EAAC;IAClC,CAAC,CAAC,CAACK,IAAI,CAAC,IAAI,CAAC;IAEb,MAAMC,KAAK;MAAA,8BAAG,aAAY;QACxB,MAAMC,MAAM,GAAG5C,cAAc,CAACF,IAAI,CAAC4C,IAAI,CAAC1B,KAAK,EAAE,KAAK,CAAC,EAAEK,IAAI,CAACwB,SAAS,CAAC;QACtE,MAAM3C,QAAQ,CAAC4C,QAAQ,CAAC9B,KAAK,EAAElB,IAAI,CAACiD,OAAO,CAACH,MAAM,CAAC,CAAC;QACpD,OAAO;UACLA,MAAM;UACNI,KAAK,EAAE;QACT,CAAC;MACH,CAAC;MAAA,gBAPKL,KAAK;QAAA;MAAA;IAAA,GAOV;IAED,MAAMM,QAAQ;MAAA,8BAAG,WAAOC,GAAG,EAAK;QAC9B,IAAI,CAACA,GAAG,CAACF,KAAK,EAAE;UACd,OAAOxC,MAAM,CAAC0C,GAAG,CAACN,MAAM,CAAC;QAC3B;MACF,CAAC;MAAA,gBAJKK,QAAQ;QAAA;MAAA;IAAA,GAIb;IAED,MAAME,KAAK;MAAA,8BAAG,WAAOD,GAAG,EAAK;QAC3B,MAAMtD,EAAE,CAACwD,SAAS,CAACF,GAAG,CAACN,MAAM,EAAET,QAAQ,EAAE;UAAEkB,IAAI,EAAE;QAAK,CAAC,CAAC;QACxD,MAAMnD,QAAQ,CAAC4C,QAAQ,CAAC9B,KAAK,EAAElB,IAAI,CAACiD,OAAO,CAACzB,MAAM,CAAC,CAAC;QACpD;QACA;QACA,MAAMhB,QAAQ,CAAC4C,GAAG,CAACN,MAAM,EAAEtB,MAAM,CAAC;QAClC4B,GAAG,CAACF,KAAK,GAAG,IAAI;QAChB,IAAI;UACF,MAAM9C,QAAQ,CAACoD,MAAM,CAACtC,KAAK,EAAEM,MAAM,CAAC;QACtC,CAAC,CAAC,OAAOiC,GAAG,EAAE;UACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;YACzB,MAAMqC,GAAG;UACX;QACF;MACF,CAAC;MAAA,gBAdKJ,KAAK;QAAA;MAAA;IAAA,GAcV;;IAED;IACA,MAAMD,GAAG,SAASP,KAAK,EAAE;IACzB,IAAI;MACF,MAAMQ,KAAK,CAACD,GAAG,CAAC;IAClB,CAAC,SAAS;MACR,MAAMD,QAAQ,CAACC,GAAG,CAAC;IACrB;;IAEA;IACA;IACA;IACA;IACA;IACA,OAAOxB,UAAU,CAAC8B,OAAO,EAAE,CAACpB,GAAG,CAAEP,KAAK,IAAK4B,WAAW,CAACzC,KAAK,EAAEa,KAAK,EAAE,IAAI,CAAC,CAAC;EAC7E,CAAC;EAAA;AAAA;AAEDlB,MAAM,CAACC,OAAO,CAAC8C,MAAM,GAAGA,MAAM;AAAA,SAEfA,MAAM;EAAA;AAAA;AAAA;EAAA,4BAArB,WAAuB1C,KAAK,EAAEC,GAAG,EAAEa,SAAS,EAAET,IAAI,GAAG,CAAC,CAAC,EAAE;IACvD,MAAM;MAAEsC,QAAQ;MAAEC;IAAK,CAAC,GAAGvC,IAAI;IAC/B,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,MAAMY,KAAK,GAAG;MACZZ,GAAG;MACHa,SAAS,EAAEA,SAAS,IAAI/B,IAAI,CAACwC,SAAS,CAACT,SAAS,CAAC;MACjD+B,IAAI,EAAEC,IAAI,CAACC,GAAG,EAAE;MAChBH,IAAI;MACJD;IACF,CAAC;IACD,IAAI;MACF,MAAMzD,QAAQ,CAAC4C,QAAQ,CAAC9B,KAAK,EAAElB,IAAI,CAACiD,OAAO,CAACzB,MAAM,CAAC,CAAC;MACpD,MAAMe,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;MACzC;MACA;MACA;MACA;MACA;MACA;MACA;MACA;MACA,MAAMjC,EAAE,CAACoE,UAAU,CAAC1C,MAAM,EAAG,KAAImB,SAAS,CAACJ,WAAW,CAAE,KAAIA,WAAY,EAAC,CAAC;MAC1E,MAAMnC,QAAQ,CAACoD,MAAM,CAACtC,KAAK,EAAEM,MAAM,CAAC;IACtC,CAAC,CAAC,OAAOiC,GAAG,EAAE;MACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO+C,SAAS;MAClB;MAEA,MAAMV,GAAG;MACT;MACA;MACA;MACA;MACA;IACF;;IACA,OAAOE,WAAW,CAACzC,KAAK,EAAEa,KAAK,CAAC;EAClC,CAAC;EAAA;AAAA;AAEDlB,MAAM,CAACC,OAAO,CAAC8C,MAAM,CAAChD,IAAI,GAAGwD,UAAU;AAEvC,SAASA,UAAU,CAAElD,KAAK,EAAEC,GAAG,EAAEa,SAAS,EAAET,IAAI,GAAG,CAAC,CAAC,EAAE;EACrD,MAAM;IAAEsC,QAAQ;IAAEC;EAAK,CAAC,GAAGvC,IAAI;EAC/B,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;EACrC,MAAMY,KAAK,GAAG;IACZZ,GAAG;IACHa,SAAS,EAAEA,SAAS,IAAI/B,IAAI,CAACwC,SAAS,CAACT,SAAS,CAAC;IACjD+B,IAAI,EAAEC,IAAI,CAACC,GAAG,EAAE;IAChBH,IAAI;IACJD;EACF,CAAC;EACDzD,QAAQ,CAAC4C,QAAQ,CAACpC,IAAI,CAACM,KAAK,EAAElB,IAAI,CAACiD,OAAO,CAACzB,MAAM,CAAC,CAAC;EACnD,MAAMe,WAAW,GAAGC,IAAI,CAACC,SAAS,CAACV,KAAK,CAAC;EACzCjC,EAAE,CAACuE,cAAc,CAAC7C,MAAM,EAAG,KAAImB,SAAS,CAACJ,WAAW,CAAE,KAAIA,WAAY,EAAC,CAAC;EACxE,IAAI;IACFnC,QAAQ,CAACoD,MAAM,CAAC5C,IAAI,CAACM,KAAK,EAAEM,MAAM,CAAC;EACrC,CAAC,CAAC,OAAOiC,GAAG,EAAE;IACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;MACzB,MAAMqC,GAAG;IACX;EACF;EACA,OAAOE,WAAW,CAACzC,KAAK,EAAEa,KAAK,CAAC;AAClC;AAEAlB,MAAM,CAACC,OAAO,CAACoB,IAAI,GAAGA,IAAI;AAAA,SAEXA,IAAI;EAAA;AAAA;AAAA;EAAA,0BAAnB,WAAqBhB,KAAK,EAAEC,GAAG,EAAE;IAC/B,MAAMK,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;IACrC,IAAI;MACF,MAAMO,OAAO,SAASC,aAAa,CAACH,MAAM,CAAC;MAC3C,OAAOE,OAAO,CAAC4C,MAAM,CAAC,CAACC,MAAM,EAAEC,IAAI,KAAK;QACtC,IAAIA,IAAI,IAAIA,IAAI,CAACrD,GAAG,KAAKA,GAAG,EAAE;UAC5B,OAAOwC,WAAW,CAACzC,KAAK,EAAEsD,IAAI,CAAC;QACjC,CAAC,MAAM;UACL,OAAOD,MAAM;QACf;MACF,CAAC,EAAE,IAAI,CAAC;IACV,CAAC,CAAC,OAAOd,GAAG,EAAE;MACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;QACzB,OAAO,IAAI;MACb,CAAC,MAAM;QACL,MAAMqC,GAAG;MACX;IACF;EACF,CAAC;EAAA;AAAA;AAED5C,MAAM,CAACC,OAAO,CAACoB,IAAI,CAACtB,IAAI,GAAG6D,QAAQ;AAEnC,SAASA,QAAQ,CAAEvD,KAAK,EAAEC,GAAG,EAAE;EAC7B,MAAMK,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;EACrC,IAAI;IACF,OAAOuD,iBAAiB,CAAClD,MAAM,CAAC,CAAC8C,MAAM,CAAC,CAACC,MAAM,EAAEC,IAAI,KAAK;MACxD,IAAIA,IAAI,IAAIA,IAAI,CAACrD,GAAG,KAAKA,GAAG,EAAE;QAC5B,OAAOwC,WAAW,CAACzC,KAAK,EAAEsD,IAAI,CAAC;MACjC,CAAC,MAAM;QACL,OAAOD,MAAM;MACf;IACF,CAAC,EAAE,IAAI,CAAC;EACV,CAAC,CAAC,OAAOd,GAAG,EAAE;IACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;MACzB,OAAO,IAAI;IACb,CAAC,MAAM;MACL,MAAMqC,GAAG;IACX;EACF;AACF;AAEA5C,MAAM,CAACC,OAAO,CAAC6D,MAAM,GAAGC,GAAG;AAE3B,SAASA,GAAG,CAAE1D,KAAK,EAAEC,GAAG,EAAEI,IAAI,GAAG,CAAC,CAAC,EAAE;EACnC,IAAI,CAACA,IAAI,CAACsD,WAAW,EAAE;IACrB,OAAOjB,MAAM,CAAC1C,KAAK,EAAEC,GAAG,EAAE,IAAI,EAAEI,IAAI,CAAC;EACvC;EAEA,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;EACrC,OAAOT,MAAM,CAACc,MAAM,CAAC;AACvB;AAEAX,MAAM,CAACC,OAAO,CAAC6D,MAAM,CAAC/D,IAAI,GAAGkE,OAAO;AAEpC,SAASA,OAAO,CAAE5D,KAAK,EAAEC,GAAG,EAAEI,IAAI,GAAG,CAAC,CAAC,EAAE;EACvC,IAAI,CAACA,IAAI,CAACsD,WAAW,EAAE;IACrB,OAAOT,UAAU,CAAClD,KAAK,EAAEC,GAAG,EAAE,IAAI,EAAEI,IAAI,CAAC;EAC3C;EAEA,MAAMC,MAAM,GAAGC,UAAU,CAACP,KAAK,EAAEC,GAAG,CAAC;EACrC,OAAOT,MAAM,CAACE,IAAI,CAACY,MAAM,CAAC;AAC5B;AAEAX,MAAM,CAACC,OAAO,CAACiE,QAAQ,GAAGA,QAAQ;AAElC,SAASA,QAAQ,CAAE7D,KAAK,EAAE;EACxB,MAAM8D,QAAQ,GAAGC,SAAS,CAAC/D,KAAK,CAAC;EACjC,MAAMgE,MAAM,GAAG,IAAInF,QAAQ,CAAC;IAAEoF,UAAU,EAAE;EAAK,CAAC,CAAC;;EAEjD;EACAC,OAAO,CAACC,OAAO,EAAE,CAACC,IAAI,iCAAC,aAAY;IACjC,MAAMC,OAAO,SAASC,cAAc,CAACR,QAAQ,CAAC;IAC9C,MAAMI,OAAO,CAACK,GAAG,CAACF,OAAO,CAACjD,GAAG;MAAA,8BAAC,WAAOd,MAAM,EAAK;QAC9C,MAAMC,UAAU,GAAGzB,IAAI,CAAC4C,IAAI,CAACoC,QAAQ,EAAExD,MAAM,CAAC;QAC9C,MAAMkE,UAAU,SAASF,cAAc,CAAC/D,UAAU,CAAC;QACnD,MAAM2D,OAAO,CAACK,GAAG,CAACC,UAAU,CAACpD,GAAG;UAAA,8BAAC,WAAOqD,SAAS,EAAK;YACpD,MAAMC,aAAa,GAAG5F,IAAI,CAAC4C,IAAI,CAACnB,UAAU,EAAEkE,SAAS,CAAC;;YAEtD;YACA,MAAME,gBAAgB,SAASL,cAAc,CAACI,aAAa,CAAC;YAC5D,MAAMR,OAAO,CAACK,GAAG,CAACI,gBAAgB,CAACvD,GAAG;cAAA,8BAAC,WAAOP,KAAK,EAAK;gBACtD,MAAM+D,SAAS,GAAG9F,IAAI,CAAC4C,IAAI,CAACgD,aAAa,EAAE7D,KAAK,CAAC;gBACjD,IAAI;kBACF,MAAML,OAAO,SAASC,aAAa,CAACmE,SAAS,CAAC;kBAC9C;kBACA;kBACA,MAAMC,OAAO,GAAGrE,OAAO,CAAC4C,MAAM,CAAC,CAAC0B,GAAG,EAAEjE,KAAK,KAAK;oBAC7CiE,GAAG,CAACC,GAAG,CAAClE,KAAK,CAACZ,GAAG,EAAEY,KAAK,CAAC;oBACzB,OAAOiE,GAAG;kBACZ,CAAC,EAAE,IAAIE,GAAG,EAAE,CAAC;kBACb;kBACA,KAAK,MAAMnE,KAAK,IAAIgE,OAAO,CAACI,MAAM,EAAE,EAAE;oBACpC,MAAMC,SAAS,GAAGzC,WAAW,CAACzC,KAAK,EAAEa,KAAK,CAAC;oBAC3C,IAAIqE,SAAS,EAAE;sBACblB,MAAM,CAAC7B,KAAK,CAAC+C,SAAS,CAAC;oBACzB;kBACF;gBACF,CAAC,CAAC,OAAO3C,GAAG,EAAE;kBACZ,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,EAAE;oBACzB,OAAO+C,SAAS;kBAClB;kBACA,MAAMV,GAAG;gBACX;cACF,CAAC;cAAA;gBAAA;cAAA;YAAA,IAAC,CAAC;UACL,CAAC;UAAA;YAAA;UAAA;QAAA,IAAC,CAAC;MACL,CAAC;MAAA;QAAA;MAAA;IAAA,IAAC,CAAC;IACHyB,MAAM,CAACmB,GAAG,EAAE;IACZ,OAAOnB,MAAM;EACf,CAAC,EAAC,CAACoB,KAAK,CAAC7C,GAAG,IAAIyB,MAAM,CAACqB,IAAI,CAAC,OAAO,EAAE9C,GAAG,CAAC,CAAC;EAE1C,OAAOyB,MAAM;AACf;AAEArE,MAAM,CAACC,OAAO,CAAC0F,EAAE,GAAGA,EAAE;AAAA,SAEPA,EAAE;EAAA;AAAA;AAAA;EAAA,wBAAjB,WAAmBtF,KAAK,EAAE;IACxB,MAAMQ,OAAO,SAASqD,QAAQ,CAAC7D,KAAK,CAAC,CAACuF,OAAO,EAAE;IAC/C,OAAO/E,OAAO,CAAC4C,MAAM,CAAC,CAAC0B,GAAG,EAAEU,EAAE,KAAK;MACjCV,GAAG,CAACU,EAAE,CAACvF,GAAG,CAAC,GAAGuF,EAAE;MAChB,OAAOV,GAAG;IACZ,CAAC,EAAE,CAAC,CAAC,CAAC;EACR,CAAC;EAAA;AAAA;AAEDnF,MAAM,CAACC,OAAO,CAACa,aAAa,GAAGA,aAAa;AAAA,SAE7BA,aAAa;EAAA;AAAA;AAAA;EAAA,oCAA5B,WAA8BH,MAAM,EAAEmF,MAAM,EAAE;IAC5C,MAAMC,IAAI,SAAS9G,EAAE,CAAC+G,QAAQ,CAACrF,MAAM,EAAE,MAAM,CAAC;IAC9C,OAAOsF,cAAc,CAACF,IAAI,EAAED,MAAM,CAAC;EACrC,CAAC;EAAA;AAAA;AAED9F,MAAM,CAACC,OAAO,CAACa,aAAa,CAACf,IAAI,GAAG8D,iBAAiB;AAErD,SAASA,iBAAiB,CAAElD,MAAM,EAAEmF,MAAM,EAAE;EAC1C,MAAMC,IAAI,GAAG9G,EAAE,CAACiH,YAAY,CAACvF,MAAM,EAAE,MAAM,CAAC;EAC5C,OAAOsF,cAAc,CAACF,IAAI,EAAED,MAAM,CAAC;AACrC;AAEA,SAASG,cAAc,CAAEF,IAAI,EAAED,MAAM,EAAE;EACrC,MAAMjF,OAAO,GAAG,EAAE;EAClBkF,IAAI,CAACI,KAAK,CAAC,IAAI,CAAC,CAACC,OAAO,CAAElF,KAAK,IAAK;IAClC,IAAI,CAACA,KAAK,EAAE;MACV;IACF;IAEA,MAAMmF,MAAM,GAAGnF,KAAK,CAACiF,KAAK,CAAC,IAAI,CAAC;IAChC,IAAI,CAACE,MAAM,CAAC,CAAC,CAAC,IAAIvE,SAAS,CAACuE,MAAM,CAAC,CAAC,CAAC,CAAC,KAAKA,MAAM,CAAC,CAAC,CAAC,EAAE;MACpD;MACA;MACA;IACF;IACA,IAAIC,GAAG;IACP,IAAI;MACFA,GAAG,GAAG3E,IAAI,CAAC4E,KAAK,CAACF,MAAM,CAAC,CAAC,CAAC,CAAC;IAC7B,CAAC,CAAC,OAAOG,CAAC,EAAE;MACV;MACA;IACF;IACA,IAAIF,GAAG,EAAE;MACPzF,OAAO,CAAC4F,IAAI,CAACH,GAAG,CAAC;IACnB;EACF,CAAC,CAAC;EACF,OAAOzF,OAAO;AAChB;AAEAb,MAAM,CAACC,OAAO,CAACmE,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAAS,CAAE/D,KAAK,EAAE;EACzB,OAAOlB,IAAI,CAAC4C,IAAI,CAAC1B,KAAK,EAAG,UAASZ,MAAO,EAAC,CAAC;AAC7C;AAEAO,MAAM,CAACC,OAAO,CAACW,UAAU,GAAGA,UAAU;AAEtC,SAASA,UAAU,CAAEP,KAAK,EAAEC,GAAG,EAAE;EAC/B,MAAMoG,MAAM,GAAGC,OAAO,CAACrG,GAAG,CAAC;EAC3B,OAAOnB,IAAI,CAAC4C,IAAI,CAAC6E,KAAK,CACpBzH,IAAI,EACJ,CAACiF,SAAS,CAAC/D,KAAK,CAAC,CAAC,CAACwG,MAAM,CAACrH,cAAc,CAACkH,MAAM,CAAC,CAAC,CAClD;AACH;AAEA1G,MAAM,CAACC,OAAO,CAAC0G,OAAO,GAAGA,OAAO;AAEhC,SAASA,OAAO,CAAErG,GAAG,EAAE;EACrB,OAAOuB,IAAI,CAACvB,GAAG,EAAE,QAAQ,CAAC;AAC5B;AAEAN,MAAM,CAACC,OAAO,CAAC6B,SAAS,GAAGA,SAAS;AAEpC,SAASA,SAAS,CAAEgF,GAAG,EAAE;EACvB,OAAOjF,IAAI,CAACiF,GAAG,EAAE,MAAM,CAAC;AAC1B;AAEA,SAASjF,IAAI,CAAEiF,GAAG,EAAEC,MAAM,EAAE;EAC1B,OAAO/H,MAAM,CACVgI,UAAU,CAACD,MAAM,CAAC,CAClBE,MAAM,CAACH,GAAG,CAAC,CACXC,MAAM,CAAC,KAAK,CAAC;AAClB;AAEA,SAASjE,WAAW,CAAEzC,KAAK,EAAEa,KAAK,EAAEgG,OAAO,EAAE;EAC3C;EACA,IAAI,CAAChG,KAAK,CAACC,SAAS,IAAI,CAAC+F,OAAO,EAAE;IAChC,OAAO,IAAI;EACb;EAEA,OAAO;IACL5G,GAAG,EAAEY,KAAK,CAACZ,GAAG;IACda,SAAS,EAAED,KAAK,CAACC,SAAS;IAC1BhC,IAAI,EAAE+B,KAAK,CAACC,SAAS,GAAG7B,WAAW,CAACe,KAAK,EAAEa,KAAK,CAACC,SAAS,CAAC,GAAGmC,SAAS;IACvEL,IAAI,EAAE/B,KAAK,CAAC+B,IAAI;IAChBC,IAAI,EAAEhC,KAAK,CAACgC,IAAI;IAChBF,QAAQ,EAAE9B,KAAK,CAAC8B;EAClB,CAAC;AACH;AAEA,SAAS2B,cAAc,CAAEwC,GAAG,EAAE;EAC5B,OAAOlI,EAAE,CAACmI,OAAO,CAACD,GAAG,CAAC,CAAC1B,KAAK,CAAE7C,GAAG,IAAK;IACpC,IAAIA,GAAG,CAACrC,IAAI,KAAK,QAAQ,IAAIqC,GAAG,CAACrC,IAAI,KAAK,SAAS,EAAE;MACnD,OAAO,EAAE;IACX;IAEA,MAAMqC,GAAG;EACX,CAAC,CAAC;AACJ"},"metadata":{},"sourceType":"script","externalDependencies":[]}